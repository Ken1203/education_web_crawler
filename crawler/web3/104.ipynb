{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options()\n",
    "opt.add_argument('--headless')\n",
    "chrome_driver_path = \"/Users/weichenho/Desktop/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = chrome_driver_path ,chrome_options=opt )\n",
    "\n",
    "driver.get('https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword=python&jobcatExpansionType=0&area=6001001000&order=15&asc=0&page=2&mode=s&jobsource=2018indexpoc')\n",
    "time.sleep(1)\n",
    "html = driver.page_source\n",
    "soup = bs(html,'lxml')\n",
    "data = soup.select('#js-job-header > div.b-float-right > label:nth-child(1) > select > option')\n",
    "p = data[-1].get('value')\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:35<00:00,  2.62it/s]1782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "href = []\n",
    "for pg in tqdm(range(1,int(p) + 1)):\n",
    "    res = requests.get(f'https://www.104.com.tw/jobs/search/?ro=0&keyword=python&jobcatExpansionType=0&order=15&asc=0&page={pg}&mode=s&jobsource=2018indexpoc')\n",
    "    soup = bs(res.text,'lxml')\n",
    "    x = soup.select('#js-job-content > article > div.b-block__left > h2 > a')\n",
    "    for i in x:\n",
    "        if  'javascript:void(0);' not in i.get('href'):\n",
    "            href.append('https:' + i.get('href'))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(len(href))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1867/1867 [1:19:21<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = Options()\n",
    "opt.add_argument('--headless')\n",
    "\n",
    "chrome_driver_path = \"/Users/weichenho/Desktop/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = chrome_driver_path ,chrome_options=opt )\n",
    "\n",
    "\n",
    "job = []\n",
    "company = []\n",
    "content =[]\n",
    "pay = []\n",
    "pay_a = []      #經常性薪資資\n",
    "loc = []\n",
    "exp  = []    #工作經驗\n",
    "lang = []    #語言能力\n",
    "tool = []   #擅長工具\n",
    "skill = []  #技能\n",
    "other = []  #其他\n",
    "\n",
    "for i in tqdm(href):\n",
    "    try :\n",
    "        driver.get(i)\n",
    "    except:\n",
    "        continue\n",
    "    time.sleep(2)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html,'lxml')\n",
    "\n",
    "    #職缺    job\n",
    "    data = soup.select('#app > div.job-header > div:nth-child(2) > div > div > div.job-header__title > h1')\n",
    "    try:\n",
    "        job.append(data[0].get('title'))\n",
    "    except:\n",
    "        job.append('NaN')\n",
    "    #公司   company\n",
    "    data = soup.select('#app > div.job-header > div:nth-child(2) > div > div > div.job-header__title > div > a:nth-child(1)')\n",
    "    try:\n",
    "        company.append(data[0].get('title')) \n",
    "    except:\n",
    "        company.append('NaN')\n",
    "\n",
    "    #內容     content\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-description > div.job-description-table.row > div.job-description.col-12 > p')\n",
    "    try:\n",
    "        if '\\n' and '\\r' in data[0].text:\n",
    "            content.append(data[0].text.replace('\\n','').replace('\\r',''))\n",
    "        elif '\\n' in  data[0].text or '\\r' not in data[0].text:\n",
    "            content.append(data[0].text.replace('\\n',''))\n",
    "        else:\n",
    "            content.append('NaN')\n",
    "    except:\n",
    "        content.append('NaN')\n",
    "    #薪資   pay\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-description > div.job-description-table.row > div:nth-child(3) > div.col.p-0.job-description-table__data > div > p.t3.mb-0.mr-2.monthly-salary.text-primary.font-weight-bold.float-left')\n",
    "    try:\n",
    "        pay.append(data[0].text)\n",
    "    except:\n",
    "        pay.append('NaN')\n",
    "    #經常性薪資 pay_a\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-description > div.job-description-table.row > div:nth-child(3) > div.col.p-0.job-description-table__data > div > p.t3.mb-0.monthly-salary-remark.text-gray-deep-dark')\n",
    "    try:\n",
    "        pay_a.append(data[0].text.replace('（','').replace('）','').replace(' ',''))\n",
    "    except:\n",
    "        pay_a.append('NaN')\n",
    "    #上班地點 loc\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-description > div.job-description-table.row > div:nth-child(5) > div.col.p-0.job-description-table__data > p')\n",
    "    try:\n",
    "        loc.append(data[0].text.replace(' ',''))\n",
    "    except:\n",
    "        loc.append('NaN')\n",
    "    #工作經驗  exp\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-requirement > div.job-requirement-table.row > div:nth-child(2) > div.col.p-0.job-requirement-table__data > p')\n",
    "    try:\n",
    "        exp.append(data[0].text)\n",
    "    except:\n",
    "        exp.append('NaN')\n",
    "    #語言能力 lang\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-requirement > div.job-requirement-table.row > div:nth-child(5) > div.col.p-0.job-requirement-table__data > p')\n",
    "    try:\n",
    "        lang.append(data[0].text)\n",
    "    except:\n",
    "        lang.append('NaN')\n",
    "    #擅長工具  tool\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-requirement > div.job-requirement-table.row > div:nth-child(6) > div.col.p-0.job-requirement-table__data > p > span > a > u')\n",
    "    t = []\n",
    "    w = ' / '\n",
    "    if data != []:\n",
    "        for i in data:\n",
    "            try:\n",
    "                t.append(i.text)\n",
    "            except:\n",
    "                t.append('NaN')\n",
    "        tool.append(w.join(t))\n",
    "    else:\n",
    "        tool.append('NaN')\n",
    "\n",
    "    #工作技能 skill\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-requirement > div.job-requirement-table.row > div:nth-child(7) > div.col.p-0.job-requirement-table__data > p > span > a > u')\n",
    "    s = []\n",
    "    if data != []:\n",
    "        for i in data:\n",
    "            try:\n",
    "                s.append(i.text)\n",
    "            except:\n",
    "                s.append('NaN')\n",
    "        skill.append(w.join(s))\n",
    "    else:\n",
    "        skill.append('NaN')\n",
    "\n",
    "\n",
    "    #其他條件 other\n",
    "    data = soup.select('#app > div.container.jb-container.pt-4.position-relative > div > div.col.main > div.dialog.container-fluid.bg-white.py-6.mb-4.rounded.job-requirement > div.job-requirement.col.opened > div > div.col.p-0.job-requirement-table__data > p')\n",
    "    try:\n",
    "        other.append(data[0].text)\n",
    "    except:\n",
    "        other.append('NaN')\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dic = {\n",
    "    '工作':job,\n",
    "    '公司':company,\n",
    "    '內容':content,\n",
    "    '薪資':pay,\n",
    "    '經常性薪資':pay_a,      #經常性薪資資\n",
    "    '地點':loc,\n",
    "    '經驗':exp,    #工作經驗\n",
    "    '語言':lang,    #語言能力\n",
    "    '工具':tool,   #擅長工具\n",
    "    '技能':skill,  #技能\n",
    "    '其他':other  #其他\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.to_csv('104.csv',index=True, index_label= 'id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新增成功\n"
     ]
    }
   ],
   "source": [
    "#存入資料，DataFrame寫入不需要新建table，須先  pip    sqlalchemy   和  pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# 初始化資料庫連接，使用pymysql模塊 # MySQL的用戶：root, 密碼:147369, 埠：3306,資料庫：test1\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:root@localhost:3306/neildb') \n",
    "df = pd.read_csv('104.csv')\n",
    "# 將新建的DataFrame儲存為MySQL中的數據表，不儲存index列， 直接打上要建立的table名就可建立，'replace'如果同名表存在就替換掉\n",
    "\n",
    "\n",
    "df.to_sql('job104', engine, index= False, if_exists='replace') \n",
    "\n",
    "print('新增成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_DB = 'neildb'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASS = 'root'\n",
    "\n",
    "def connect_mysql():  #連線資料庫\n",
    "    global connect, cursor\n",
    "    connect = pymysql.connect(host = MYSQL_HOST, db = MYSQL_DB, user = MYSQL_USER, password = MYSQL_PASS,\n",
    "            charset = 'utf8', use_unicode = True)\n",
    "    cursor = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Java ', ' Python ', ' MySQL ', ' PostgreSQL']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect_mysql()\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM job104', con = connect) #使用connect指定的Mysql獲取資料\n",
    "data = df.to_dict('recode')\n",
    "result = []             #有填資料的比數約 996\n",
    "for i in data:\n",
    "    if i['工具'] != None:\n",
    "        p = i['工具'].split('/')\n",
    "        result.append(p)\n",
    "\n",
    "r2 = []    #關鍵字出現的總資料    #48xx\n",
    "for i in r:\n",
    "    r2.append(i.replace(' ',''))\n",
    "\n",
    "r3 = []   #全部分類（不包含重複出現）\n",
    "for i in r2:\n",
    "    if i not in r3:\n",
    "        r3.append(i)\n",
    "\n",
    "list1 = ['Python', 'MySQL', 'HTML', 'CSS', 'Django', 'AJAX', 'PHP', 'JavaScript', 'jQuery', 'Java', 'Github', 'SQL', 'Git', 'Tableau', 'Hive', 'Spark'] #自定義\n",
    "\n",
    "num = {}  #總數\n",
    "\n",
    "for i in list1:\n",
    "    num[i] = 0\n",
    "\n",
    "for i in num:\n",
    "    for j in r2:\n",
    "        if j == i:\n",
    "            num[i] = num[i] + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#1111111111111111111111111111111111111111111111111111111111111111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 764/764 [15:08<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "opt = Options()                   #無視窗套件\n",
    "opt.add_argument('--headless')    #無視窗套件\n",
    "opt.add_argument(\"window-size=1920x1080\")  #無視窗套件（下拉需設定視窗大小）\n",
    "\n",
    "chrome_driver_path = \"/Users/chih-liangyang/Downloads/chromedriver\"\n",
    "# driver = webdriver.Chrome(executable_path = chrome_driver_path ,chrome_options=opt )  #無視窗套件\n",
    "driver = webdriver.Chrome(chrome_driver_path)\n",
    "\n",
    "driver.get('http://www.1111edu.com.tw/advancedStudies_courseList.php?nowpage=1&cate2=0&adSearchAddress=&adSearchTime=&adSearchCdate=&keyword=&page_sort_col=&page_sort_order=#news-wrapper')\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[@id='pager']/ul/li[12]/a\").click()\n",
    "time.sleep(2)\n",
    "html = driver.page_source\n",
    "soup = bs(html,'lxml')\n",
    "data = soup.select('#pager > ul > li > a')\n",
    "page = data[-2].text\n",
    "\n",
    "driver.close()\n",
    "\n",
    "\n",
    "def dtime():\n",
    "    return datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "all_class = []\n",
    "\n",
    "for i in tqdm(range(1, int(page) + 1)):\n",
    "    res = requests.get(f'http://www.1111edu.com.tw/advancedStudies_courseList.php?nowpage={i}&cate2=0&adSearchAddress=&adSearchTime=&adSearchCdate=&keyword=&page_sort_col=&page_sort_order=#news-wrapper')\n",
    "    soup = bs(res.text,'lxml')\n",
    "\n",
    "    title = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(1) > a')\n",
    "    school = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(2) > a')\n",
    "    location = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(3) > strong')\n",
    "    price = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(4)')\n",
    "    date = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(5) > strong')\n",
    "    href = soup.select('#mainContent > div > div.civtableS01 > table > tbody > tr > td:nth-child(1) > a')\n",
    "    \n",
    "    data = zip(title, school, location, price, date ,href)\n",
    "\n",
    "    for title, school, location, price, date ,href in data:\n",
    "        dic = {}\n",
    "        dic['web'] = '1111進修網'\n",
    "        dic['today'] = dtime()\n",
    "        dic['title'] = title.text\n",
    "        dic['school'] = school.text\n",
    "        dic['location'] = location.text\n",
    "        dic['price'] = price.text\n",
    "        dic['date'] = date.text\n",
    "        dic['href'] = 'http://www.1111edu.com.tw/' + href.get('href')\n",
    "\n",
    "        all_class.append(dic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: use options instead of chrome_options\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import time\n",
    "try:\n",
    "    chrome_options= Options()                   #無視窗套件\n",
    "    chrome_options.add_argument('--headless')    #無視窗套件\n",
    "    chrome_options.add_argument(\"window-size=1920x1080\")  #無視窗套件（下拉需設定視窗大小）\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_driver_path = \"/Users/chih-liangyang/Downloads/chromedriver\"\n",
    "    # driver = webdriver.Chrome(executable_path = chrome_driver_path ,chrome_options=opt )  #無視窗套件\n",
    "    driver = webdriver.Chrome(chrome_driver_path, chrome_options=chrome_options)\n",
    "    driver.get('http://www.1111edu.com.tw/advancedStudies_courseList.php?nowpage=1&cate2=0&adSearchAddress=&adSearchTime=&adSearchCdate=&keyword=&page_sort_col=&page_sort_order=#news-wrapper')\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.find_elements_by_xpath(\"//*[@id='pager']/ul/li[12]/a\")\n",
    "#     driver.get(hrf)\n",
    "#     driver.implicitly_wait(2)\n",
    "#     html = driver.page_source\n",
    "#     soup = bs(html,'lxml')\n",
    "#     data = soup.select('#pager > ul > li > a')\n",
    "#     page = data[-2].text\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "finally: \n",
    "#     print(page)\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新增成功\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_class)\n",
    "\n",
    "#engine = create_engine('mysql+pymysql://root:root@localhost:3306/neildb') \n",
    "engine = create_engine('mysql+pymysql://ken:3989889@localhost:3306/test') \n",
    "# 將新建的DataFrame儲存為MySQL中的數據表，不儲存index列， 直接打上要建立的table名就可建立，'replace'如果同名表存在就替換掉\n",
    "\n",
    "\n",
    "df.to_sql('school1111', engine, index= False, if_exists='replace') \n",
    "\n",
    "print('新增成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_DB = 'neildb'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASS = 'root'\n",
    "\n",
    "def connect_mysql():  #連線資料庫\n",
    "    global connect, cursor\n",
    "    connect = pymysql.connect(host = MYSQL_HOST, db = MYSQL_DB, user = MYSQL_USER, password = MYSQL_PASS,\n",
    "            charset = 'utf8', use_unicode = True)\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "connect_mysql()\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM school1111', con = connect) #使用connect指定的Mysql獲取資料\n",
    "data = df.to_dict('recode')\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "\n",
    "'web':'六角學院'\n",
    "'today':'2020-07-03'\n",
    "'title':'課程名稱'\n",
    "'price':'2500'\n",
    "'hours':'1.5'\n",
    "'tech':'WebDesign'\n",
    "'bz':'MKT'\n",
    "'lan':'ENG'\n",
    "'other':'True'\n",
    "'type':'online'\n",
    "'all_city':'taipei'\n",
    "'taipei_dist':'大安區'\n",
    "'address':''\n",
    "'weekday':'True'\n",
    "'weekends':'False'\n",
    "'start_time':'上課時段'\n",
    "'end_time':'下課時段'\n",
    "    \n",
    "\n",
    "Python, WebDesign, Java, C#, R, C++, Database, Network, Linux, MKT,   MGMT,     FIN,     LIS,     ENG\n",
    "                                                網路工程        行銷    商管      財務      證照      英文\n",
    "                                                                                              \n",
    "'''\n",
    "\n",
    "all_ = []\n",
    "\n",
    "for i in data:\n",
    "    dic = {}\n",
    "    dic['web'] = i['web']\n",
    "    dic['today'] = i['today']\n",
    "    dic['title'] = i['title']\n",
    "    \n",
    "    if i['price'] == '線上洽詢' or i['price'] == '電洽' or i['price'] == '補助訓練':\n",
    "        dic['price'] = np.nan\n",
    "    \n",
    "    elif i['price'] == '免費課程':\n",
    "        dic['price'] = 0   \n",
    "    \n",
    "    else:\n",
    "        dic['price'] = int(i['price'])\n",
    "        \n",
    "    dic['hours'] = np.nan\n",
    "    \n",
    "    if '前端' in i['title'] or 'PHP' in i['title'] or 'HTML' in i['title'] or 'CSS' in i['title'] or 'javaScript' in i['title'] or 'jQuery' in i['title'] or 'RWD' in i['title']:\n",
    "        dic['tech'] = 'WebDesign'\n",
    "    \n",
    "    elif 'R' in i['title'] and 'AR' not in i['title'] and 'VR' not in i['title'] and 'RE' not in i['title'] and 'Re' not in i['title']:\n",
    "        dic['tech'] = 'R'\n",
    "    \n",
    "    elif 'Python' in i['title'] or '爬蟲' in i['title'] or 'Keras' in i['title'] or 'Django' in i['title'] or 'Flask' in i['title'] or 'python' in i['title']:\n",
    "        dic['tech'] = 'Python'\n",
    "\n",
    "    elif 'Java' in i['title'] or 'Java' in i['title'] or 'JAVA' in i['title']:\n",
    "        dic['tech'] = 'Java'\n",
    "\n",
    "    elif 'C#' in i['title']:\n",
    "        dic['tech'] = 'C#'\n",
    "\n",
    "    elif 'C++' in i['title'] or 'C＋＋' in i['title']:\n",
    "        dic['tech'] = 'C++'\n",
    "\n",
    "    elif 'Database' in i['title'] or '資料庫' in i['title'] or 'SQL' in i['title'] or 'DB' in i['title']:\n",
    "        dic['tech'] = 'Database'\n",
    "\n",
    "    elif 'Linux' in i['title']:\n",
    "        dic['tech'] = 'Linux'\n",
    "\n",
    "    elif 'Network' in i['title']:\n",
    "        dic['tech'] = 'Network'\n",
    "        \n",
    "    else:\n",
    "        dic['tech'] = 'NaN' \n",
    "        \n",
    "    if '行銷' in i['title']:\n",
    "        dic['bz'] = 'MKT'\n",
    "\n",
    "    elif '商管' in i['title']:\n",
    "        dic['bz'] = 'MGMT'\n",
    "\n",
    "    elif '財務' in i['title']:\n",
    "        dic['bz'] = 'FIN'\n",
    "\n",
    "    elif '證照' in i['title']:\n",
    "        dic['bz'] = 'LIS'\n",
    "    \n",
    "    else:\n",
    "        dic['bz'] = 'NaN'\n",
    "        \n",
    "    if '英文' in i['title']:\n",
    "        dic['lan'] = 'ENG'\n",
    "    \n",
    "    else:\n",
    "        dic['lan'] = 'NaN'\n",
    "        \n",
    "    all_.append(dic)\n",
    "\n",
    "for i in all_:\n",
    "    if i['tech'] == 'NaN' and i['bz'] == 'NaN' and i['lan'] == 'NaN':\n",
    "        i['other'] = True\n",
    "    \n",
    "    else:\n",
    "        i['other'] = False\n",
    "\n",
    "for i in all_:\n",
    "    if i['tech'] == 'NaN':\n",
    "        i['tech'] = np.nan\n",
    "    \n",
    "    if i['bz'] == 'NaN':\n",
    "        i['bz'] = np.nan\n",
    "    \n",
    "    if i['lan'] == 'NaN':\n",
    "        i['lan'] = np.nan\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新增成功\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_)\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:root@localhost:3306/neildb') \n",
    "\n",
    "# 將新建的DataFrame儲存為MySQL中的數據表，不儲存index列， 直接打上要建立的table名就可建立，'replace'如果同名表存在就替換掉\n",
    "\n",
    "\n",
    "df.to_sql('school1111_wash', engine, index= False, if_exists='replace') \n",
    "\n",
    "print('新增成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
